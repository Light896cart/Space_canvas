import torch
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import transforms

from src.data.dataset import Space_dataset

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–µ—à –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (–∫–ª—é—á ‚Äî –ø—É—Ç—å –∫ CSV)
_VAL_DATASET_CACHE = {}

def create_train_val_dataloader(
        path_csv: str,
        path_img: str,
        list_label: list[str],
        train_ratio: int | None = 0.9,
        list_extra: list[str] | None = None,
        transform: transforms.Compose | None = None,
        path_val_dataset: str | None = None
):
    """
    –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∏ —Å–æ–∑–¥–∞–µ–º DataLoader

    Args:
        path_csv: –ø—É—Ç—å –¥–æ —Ñ–∞–π–ª–∞ csv
        path_img: –ø—É—Ç—å –¥–æ –ø–∞–ø–∫–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        list_label: —Å–ø–∏—Å–æ–∫ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ —Å—Ç–æ–ª–±—Ü–æ–≤ —Å –º–µ—Ç–∫–∞–º–∏
        train_ratio: –ø—Ä–æ—Ü–µ–Ω—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ—Ç –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        list_extra: —Å–ø–∏—Å–æ–∫ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ —Å—Ç–æ–ª–±—Ü–æ–≤ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        transform: —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
        path_val_dataset: –ü—É—Ç—å –¥–æ —Ñ–∞–π–ª–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    Return:

    """

    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    dataset_train = Space_dataset(
        path_csv=path_csv,
        path_img=path_img,
        list_label=list_label,
        list_extrra=list_extra,
        transform=transform
    )
    if path_val_dataset:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π –¥–∞—Ç–∞—Å–µ—Ç –≤ –∫–µ—à–µ
        cache_key = (path_val_dataset, path_img, tuple(list_label), tuple(list_extra or []), id(transform))
        if cache_key in _VAL_DATASET_CACHE:
            print(f"üîÅ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π dataset_val –¥–ª—è {path_val_dataset}")
            dataset_val = _VAL_DATASET_CACHE[cache_key]
        else:
            dataset_val = Space_dataset(
                path_csv=path_val_dataset,
                path_img=path_img,
                list_label=list_label,
                list_extrra=list_extra,
                transform=transform
            )
            _VAL_DATASET_CACHE[cache_key] = dataset_val
            print(f"‚úÖ –ó–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω –Ω–æ–≤—ã–π dataset_val –¥–ª—è {path_val_dataset}")
    else:
        # –ó–∞–¥–∞—ë–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏
        train_size = int(train_ratio * len(dataset_train))
        val_size = len(dataset_train) - train_size

        # –†–∞–∑–¥–µ–ª—è–µ–º
        dataset_train, dataset_val = random_split(dataset_train, [train_size, val_size])

    train_dataset = DataLoader(dataset_train,batch_size=32,shuffle=True)
    val_dataset = DataLoader(dataset_val,batch_size=32,shuffle=False)
    return train_dataset, val_dataset
